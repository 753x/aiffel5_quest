{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 딥러닝으로 시작하는 컴퓨터 비전\n",
    "### 7. Transfer Learning 이해하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7-1 들어가며"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img01.png)\n",
    "![](./img07/img02.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 학습 내용\n",
    "대규모 모델 학습의 어려움\n",
    "\n",
    " - ImageNet Competition의 결과를 통해 모델이 커지면서 모델이 원활하게 학습하기 위해서 필요한 부분을 알아봅시다.\n",
    "\n",
    "Transfer Learning의 아이디어\n",
    "\n",
    " - 처음부터 모델을 학습하는 것과 학습된 모델을 이용하는 것에 대한 특징을 통해 Transfer Learning의 개념과 Transfer Learning의 효과를 알아봅시다.\n",
    "\n",
    "Transfer Learning 적용\n",
    "\n",
    " - Transfer Learning을 적용할 때 어떤 것을 고려해야 하는지, 어떻게 적절하게 적용할 수 있을지에 대한 것을 알아봅시다.\n",
    "\n",
    "#### 학습 목표\n",
    "\n",
    "Transfer learning의 목적을 설명할 수 있습니다.\n",
    "\n",
    "이전에 학습된 모델을 활용할 수 있는 근거를 설명할 수 있습니다.\n",
    "\n",
    "내가 가진 데이터의 양과 도메인에 따라서 어떻게 Transfer learning을 적용해야 하는지를 이해하고 설명할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7-2. 대규모 모델 학습의 어려움"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img03.png)\n",
    "\n",
    "1000개의 클래스를 가진 이미지를 분류"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img04.png)\n",
    "\n",
    "2012년을 기준으로 딥러닝 모델이 대회를 휩씀\n",
    "\n",
    "그래프 숫자는 오분류!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img05.png)\n",
    "\n",
    "데이터의 양과 질을 모두 충족시키는 것이 중요\n",
    "\n",
    "양질의 데이터를 모으는게 딥러닝 모델을 잘 학습하고 테스트의 성능을 높이는데 핵심적인 요소\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img06.png)\n",
    "\n",
    "내가 양과 질을 만족시키는 데이터와 모델을 학습시키는데 들어가는 비용을 다 갖출수 없기때문에 거기서 부터 시작한 것이 Transfer Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. ImageNet Competition의 결과를 보며 알게 된 특징과 오분류율에 비해 파라미터 수가 어떻게 되었는지 설명해 보세요.\n",
    "\n",
    "2012년을 기점으로, 딥러닝 기반 모델들이 ImageNet competition에서 좋은 결과를 얻었습니다. ImageNet competition에서 처음으로 우승했던 딥러닝 기반 모델인 AlexNet(2012)에는 8개의 레이어가 있었습니다. 또한, VGG(19개), GoogleNet(22개)을 거쳐서, 2015년의 ResNet에는 152개의 레이어가 있습니다.\n",
    "\n",
    "→ 오분류율이 감소함과 동시에, 연산해야 하는 파라미터의 수가 급격하게 증가했습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. 우리가 딥러닝 모델을 학습 할 때 데이터를 얼마나 모아야 하는지 어떤 데이터를 모아야 성능을 높일 수 있을지 생각하고 설명해 보세요.\n",
    "\n",
    "양질의 데이터를 많이 모으는 것이 딥러닝 모델의 성능을 높이는 데 핵심적인 요소입니다. 그러나 질적인 부분을 보완하는 것도 쉽지 않고, 양적인 부분을 보완하는 것도 쉽지 않은 일입니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. 만약에 데이터도 없고, 연산 비용을 낼 만한 자금도 없고, 시간도 부족할 때, 어떻게 하면 딥러닝 모델을 잘 학습할 수 있을지 설명해 보세요.\n",
    "\n",
    "위와 같은 경우는 Transfer Learning을 사용하면 됩니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7-3. Transfer Learning의 아이디어"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img07.png)\n",
    "\n",
    "모델을 처음부터 끝까지 전부 training하는 것이 방법일까?\n",
    "\n",
    "→ 모델의 일부분만 학습시키는 것이 가능할까?\n",
    "\n",
    "모델은 파라미터의 덩어리\n",
    "\n",
    "모델 학습은 파라미터를 업데이트 하는것\n",
    "\n",
    "이미 학습된 모델을 가져다 쓰는 것을 Pre-trained model이라고 함 (Knowledge Transfer)\n",
    "\n",
    "여기서 부터 Transfer Learning의 아이디어가 시작됨"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img08.png)\n",
    "\n",
    "Pre-trained model을 사용함으로써 예측능력 향상\n",
    "\n",
    "학습시키기 어렵다 = 비용이 많이 들고 문제가 발생할 수 있다.\n",
    "\n",
    "텐서플로우에서도 VGG, ResNet, Inception 등의 Pre-trained model을 제공함"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. 딥러닝 모델을 학습할 때, 처음부터 학습하는 것 (training from scratch)과 학습된 모델 이용하는 것 (transfer learning)은 각 어떤 방법이고 어떤 특징이 있는지 설명해 보세요.\n",
    "\n",
    "<처음부터 학습하기 (training from scratch)>\n",
    "• 파라미터 초기화, 학습 등 모든 과정의 코드를 작성하고 실행시킵니다.\n",
    "• loss 값을 보면서 성능이 좋아질 때까지 모델 학습을 진행합니다.\n",
    "\n",
    "<학습된 모델 이용하기 (transfer learning)>\n",
    "• 이미 학습된 모델(pre-trained model)의 파라미터 값들을 가져다 쓰는 것(Knowledge Transfer)입니다.\n",
    "• 파라미터 전체를 그대로 사용할 수도 있고, 일부 파라미터만 학습시킬 수도 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. 위의 퀴즈를 통해 Transfer Learning에 대해서 알았습니다. 그럼 Transfer Learning을 사용했을 때 효과는 어떤 것이 있는지 설명해 보세요.\n",
    "\n",
    "기존에 만들어진(학습된) 모델을 사용하여 새로운 모델을 만들 때, 학습을 빠르게 하며 예측 능력을 더 높일 수 있습니다. 또한, 복잡한 모델일수록 학습시키기 어렵다는 난점을 해결할 수 있으며 학습에 들어가는 연산 비용을 절감할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7-4. Transfer Learning의 적용"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img09.png)\n",
    "\n",
    "인풋과 가까운쪽은 일반적인 간단한 학습을 하고 뒤로 갈수록 복잡한(task specific) 학습을 하게끔 함\n",
    "\n",
    "Domain specific은 내가 분류하고자 하는 영역 (ex. 개, 고양이, 자동차 등)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img10.png)\n",
    "\n",
    "VGG16에서 16은 연산이 필요한 레이어의 수 (파라미터 러닝이 필요한 수)\n",
    "\n",
    "데이터셋의 사이즈가 충분히 크다면 전체 학습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img11.png)\n",
    "\n",
    "데이터셋의 크기가 작다면 내가 필요한 영역만 골라 학습(나머지는 다른 사람이 학습한 모델을 그대로 가져옴)\n",
    "\n",
    "freeze : 학습을 하지 않는다는 의미 (다른 사람이 학습한 모델을 그대로 가져옴)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img12.png)\n",
    "\n",
    "데이터셋이 많지도 적지도 않은 상태라면 내가 필요한 영역만 학습하고 나머지는 freeze 시키는 것이 좋음\n",
    "\n",
    "input에 가까운 일반적인 피쳐를 다시 학습할 필요는 없으니 output에 가까운 피쳐먼저 학습하고 나머지는 freeze 시키는 것이 좋음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img13.png)\n",
    "\n",
    "데이터의 유사성 : pre-trained model의 데이터와 내 데이터가 얼마나 유사한지\n",
    "\n",
    "데이터의 사이즈 : 내가 가진 데이터의 사이즈가 얼마나 큰지\n",
    "\n",
    "데이터의 양도 많고 유사한 데이터 : Quadrant 2 - 끝부분만 학습\n",
    "\n",
    "데이터의 양은 많지만 유사하지 않은 데이터 : Quadrant 1 - 전체 학습\n",
    "\n",
    "데이터의 양은 적지만 유사한 데이터 : Quadrant 4 - 도메인이 같으니까 Task specific한 부분만 추가로 학습\n",
    "\n",
    "데이터의 양도 적고 유사하지 않은 데이터 : Quadrant 3 - 일반적인 피쳐와 관련된 부분(색, 선, 도형 형태)는 freeze 시키고 내가 가진 데이터에 관련된 부분만 학습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. Layer 별로 Feature Learning의 차이가 발생합니다. input과 가까운 레이어와 input과 멀어지는 레이어는 각각 어떤 부분이 학습되는지 설명해 보세요.\n",
    "\n",
    "input과 가까운 레이어에서는 이미지의 단순하고 일반적인 패턴을 인식하는 filter들이 학습됩니다. 레이어가 input과 멀어질수록 복잡하고 task(domain)-specific 한 특성들이 학습됩니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. VGG16 모델을 학습할 때 데이터셋의 사이즈에 따라서 어떻게 모델을 학습하면 좋을지 설명해 보세요. (데이터셋의 사이즈가 충분히 클 때, 데이터셋의 사이즈가 작을 때, 데이터셋의 사이즈가 중간 정도일 때)\n",
    "\n",
    "만약 가지고 있는 데이터셋의 사이즈가 충분히 크다면 VGG16 모델 전체를 학습시킵니다. 반면에 데이터셋의 사이즈가 작다면 feature extractor 부분의 파라미터를 고정(freeze) 시키고, 내가 가진 task 또는 데이터와 관계되는 일부 layer 부분만 학습시킵니다. 마지막으로 데이터셋의 사이즈가 중간 정도라면 freeze 하는 부분을 줄여서 학습시킵니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. 데이터셋의 유사성과 데이터셋의 사이즈에 따라서 학습방법이 각각 어떻게 달라지는지 설명해 보세요.\n",
    "\n",
    "한 줄로 요약하자면 가지고 있는 데이터셋의 크기가 작을수록, pre-training을 진행한 데이터셋과의 유사성(domain의 유사성)이 높을수록 많은 레이어를 freeze 하는 것이 효과적입니다.\n",
    "\n",
    "데이터셋의 유사성이 높고 데이터셋의 사이즈가 크다면, 전체 모델을 학습시킵니다. 데이터셋의 유사성이 높고 데이터셋의 사이즈가 작다면, feature extractor 부분의 파라미터를 고정(freeze) 시키고, 내가 가진 task 또는 데이터와 관계되는 일부 layer 부분만 학습시킵니다. 데이터셋의 유사성이 낮고 데이터셋의 사이즈가 크다면, 전체 모델을 학습시킵니다. 데이터셋의 유사성이 낮고 데이터셋의 사이즈가 작다면, feature extractor 부분의 파라미터를 고정(freeze) 시키고, 내가 가진 task 또는 데이터와 관계되는 일부 layer 부분만 학습시킵니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ResNet50 transfer learning 코드로 살펴보기\n",
    "\n",
    "준비된 CIFAR-10 small 데이터셋을 실습 폴더에 연결합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: aiffel/cifar_10_small/train/train: File exists\r\n",
      "ln: aiffel/cifar_10_small/test/test: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p aiffel/cifar_10_small\n",
    "\n",
    "!ln -s ~/data/cifar_10_small/train/ aiffel/cifar_10_small/train\n",
    "!ln -s ~/data/cifar_10_small/test/ aiffel/cifar_10_small/test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:58:48.837079Z",
     "start_time": "2023-08-16T05:58:48.445187Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "실습에 필요한 라이브러리를 불러옵니다.\n",
    "각각의 클래스에 해당하는 이미지의 개수를 알아보기 위해, 실습 데이터의 클래스마다 파일 경로를 변수로 정의합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "# import plaidml.keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections.abc import Iterable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:58:48.842124Z",
     "start_time": "2023-08-16T05:58:48.840817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train_dir='aiffel/cifar_10_small/train'\n",
    "test_dir='aiffel/cifar_10_small/test'\n",
    "\n",
    "train_aeroplane_dir= os.path.join(train_dir,'aeroplane')\n",
    "train_bird_dir=os.path.join(train_dir,'bird')\n",
    "train_car_dir= os.path.join(train_dir,'car')\n",
    "train_cat_dir=os.path.join(train_dir,'cat')\n",
    "\n",
    "test_aeroplane_dir= os.path.join(test_dir,'aeroplane')\n",
    "test_bird_dir=os.path.join(test_dir,'bird')\n",
    "test_car_dir= os.path.join(test_dir,'car')\n",
    "test_cat_dir=os.path.join(test_dir,'cat')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:58:48.847348Z",
     "start_time": "2023-08-16T05:58:48.845577Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "각각의 디렉토리에 포함된 파일의 개수를 출력하여 훈련/테스트 데이터셋의 이미지 개수를 알아냅니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 aeroplane 이미지 전체 개수: 5000\n",
      "훈련용 bird 이미지 전체 개수: 5000\n",
      "훈련용 car 이미지 전체 개수: 5000\n",
      "훈련용 cat 이미지 전체 개수: 5000\n"
     ]
    }
   ],
   "source": [
    "# 훈련용 데이터셋의 이미지 개수 출력\n",
    "print('훈련용 aeroplane 이미지 전체 개수:', len(os.listdir(train_aeroplane_dir)))\n",
    "print('훈련용 bird 이미지 전체 개수:', len(os.listdir(train_bird_dir)))\n",
    "print('훈련용 car 이미지 전체 개수:', len(os.listdir(train_car_dir)))\n",
    "print('훈련용 cat 이미지 전체 개수:', len(os.listdir(train_cat_dir)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:58:48.874112Z",
     "start_time": "2023-08-16T05:58:48.851335Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트용 aeroplane 이미지 전체 개수: 1000\n",
      "테스트용 bird 이미지 전체 개수: 1000\n",
      "테스트용 car 이미지 전체 개수: 1000\n",
      "테스트용 cat 이미지 전체 개수: 1000\n"
     ]
    }
   ],
   "source": [
    "# Q. 테스트용 데이터셋의 이미지 개수를 각 디렉토리별로 출력해 보세요.\n",
    "print('테스트용 aeroplane 이미지 전체 개수:', len(os.listdir(test_aeroplane_dir)))\n",
    "print('테스트용 bird 이미지 전체 개수:', len(os.listdir(test_bird_dir)))\n",
    "print('테스트용 car 이미지 전체 개수:', len(os.listdir(test_car_dir)))\n",
    "print('테스트용 cat 이미지 전체 개수:', len(os.listdir(test_cat_dir)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:58:48.886500Z",
     "start_time": "2023-08-16T05:58:48.874371Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 데이터 파이프 라인 생성하기\n",
    "\n",
    "데이터 파이프 라인은 본 과정의 핵심이 아니기 때문에 이런 예제가 있다는 정도만 받으들이시면 됩니다. 😊\n",
    "\n",
    "데이터를 디렉토리로부터 불러올 때 한번에 가져올 데이터의 수인 batch size를 설정하고, data generator를 생성하여 데이터를 모델에 넣을 수 있도록 합니다.\n",
    "\n",
    "우선 ImageDataGenerator 객체를 생성하여 데이터 파이프 라인을 만듭니다. train dataset의 generator에는 augmentation이 포함되고, test dataset의 generator는 원본 상태를 유지하고 rescaling만 적용합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Iterable' from 'collections' (/Users/ralphpark/anaconda3/lib/python3.10/collections/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Training 데이터의 augmentation 파이프 라인 만들기\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m augmentation_train_datagen \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocessing\u001B[49m\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mImageDataGenerator( rescale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m255\u001B[39m,  \u001B[38;5;66;03m# 모든 데이터의 값을 1/255로 스케일 조정\u001B[39;00m\n\u001B[1;32m      8\u001B[0m                                     rotation_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m40\u001B[39m,  \u001B[38;5;66;03m# 0~40도 사이로 이미지 회전\u001B[39;00m\n\u001B[1;32m      9\u001B[0m                                     width_shift_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,  \u001B[38;5;66;03m# 전체 가로 길이를 기준으로 0.2 비율까지 가로로 이동\u001B[39;00m\n\u001B[1;32m     10\u001B[0m                                     height_shift_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,   \u001B[38;5;66;03m# 전체 세로 길이를 기준으로 0.2 비율까지 가로로 이동\u001B[39;00m\n\u001B[1;32m     11\u001B[0m                                     shear_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,  \u001B[38;5;66;03m# 0.2 라디안 정도까지 이미지를 기울이기\u001B[39;00m\n\u001B[1;32m     12\u001B[0m                                     zoom_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;66;03m# 확대와 축소의 범위 [1-0.2 ~ 1+0.2 ]\u001B[39;00m\n\u001B[1;32m     13\u001B[0m                                     horizontal_flip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,)  \u001B[38;5;66;03m# 수평 기준 플립을 할 지, 하지 않을 지를 결정\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Test 데이터의 augmentation 파이프 라인 만들기\u001B[39;00m\n\u001B[1;32m     16\u001B[0m test_datagen \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mpreprocessing\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mImageDataGenerator(rescale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m255\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:58\u001B[0m, in \u001B[0;36mLazyLoader.__getattr__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, item):\n\u001B[0;32m---> 58\u001B[0m   module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, item)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:41\u001B[0m, in \u001B[0;36mLazyLoader._load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent_module_globals[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_name] \u001B[38;5;241m=\u001B[39m module\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Emit a warning if one was specified\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:992\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:992\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:992\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1006\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:688\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:883\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/keras/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m absolute_import\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/keras/utils/__init__.py:27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m to_categorical\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m normalize\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmulti_gpu_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multi_gpu_model\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/keras/utils/multi_gpu_utils.py:7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m division\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmerge\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m concatenate\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend \u001B[38;5;28;01mas\u001B[39;00m K\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Lambda\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/keras/layers/__init__.py:5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deserialize_keras_object\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Layer\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InputLayer\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InputSpec\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/keras/engine/__init__.py:8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Layer\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnetwork\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_source_inputs\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/keras/engine/training.py:21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m standardize_weights\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m weighted_masked_objective\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training_arrays\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training_generator\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend \u001B[38;5;28;01mas\u001B[39;00m K\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/keras/engine/training_arrays.py:14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_num_samples\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend \u001B[38;5;28;01mas\u001B[39;00m K\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m callbacks \u001B[38;5;28;01mas\u001B[39;00m cbks\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Progbar\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m slice_arrays\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/keras/callbacks.py:20\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deque\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OrderedDict\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Iterable\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Progbar\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend \u001B[38;5;28;01mas\u001B[39;00m K\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'Iterable' from 'collections' (/Users/ralphpark/anaconda3/lib/python3.10/collections/__init__.py)"
     ]
    }
   ],
   "source": [
    "### data 파이프 라인 생성\n",
    "\n",
    "# 데이터를 디렉토리로부터 불러올 때, 한번에 가져올 데이터의 수\n",
    "batch_size=20\n",
    "\n",
    "# Training 데이터의 augmentation 파이프 라인 만들기\n",
    "augmentation_train_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1./255,  # 모든 데이터의 값을 1/255로 스케일 조정\n",
    "                                    rotation_range=40,  # 0~40도 사이로 이미지 회전\n",
    "                                    width_shift_range=0.2,  # 전체 가로 길이를 기준으로 0.2 비율까지 가로로 이동\n",
    "                                    height_shift_range=0.2,   # 전체 세로 길이를 기준으로 0.2 비율까지 가로로 이동\n",
    "                                    shear_range=0.2,  # 0.2 라디안 정도까지 이미지를 기울이기\n",
    "                                    zoom_range=0.2, # 확대와 축소의 범위 [1-0.2 ~ 1+0.2 ]\n",
    "                                    horizontal_flip=True,)  # 수평 기준 플립을 할 지, 하지 않을 지를 결정\n",
    "\n",
    "# Test 데이터의 augmentation 파이프 라인 만들기\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T05:58:49.131262Z",
     "start_time": "2023-08-16T05:58:48.891247Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Augmentation 파이프라인을 기준으로 디렉토리로부터 데이터를 불러오는 모듈을 만듭니다. train/test dataset의 경로와 이미지 크기 등을 지정하여 train_generator와 test_generator 생성하면 됩니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Augmentation 파이프라인을 기준으로 디렉토리로부터 데이터를 불러 오는 모듈 만들기\n",
    "train_generator = augmentation_train_datagen.flow_from_directory(\n",
    "        directory=train_dir, #  어느 디렉터리에서 이미지 데이터를 가져올 것인가?\n",
    "        target_size=(150, 150), # 모든 이미지를 150 × 150 크기로 바꿉니다\n",
    "        batch_size=batch_size, # 디렉토리에서 batch size만큼의 이미지를 가져옵니다.\n",
    "        interpolation='bilinear',  # resize를 할 때, interpolatrion 기법을 결정합니다.\n",
    "        color_mode ='rgb',\n",
    "        shuffle='True', # 이미지를 셔플링할 지 하지 않을 지를 결정.\n",
    "        class_mode='categorical') # multiclass의 경우이므로 class mode는 categorical\n",
    "\n",
    "print(train_generator.class_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Q. Test 데이터 디렉토리로부터 이미지를 불러오는 파이프라인을 완성해 보세요.\n",
    "# (위의 train_generator와 조건은 동일)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        directory=test_dir, #  어느 디렉터리에서 이미지 데이터를 가져올 것인가?\n",
    "        target_size=(150, 150), # 모든 이미지를 150 × 150 크기로 바꿉니다\n",
    "        batch_size=batch_size, # 디렉토리에서 batch size만큼의 이미지를 가져옵니다.\n",
    "        interpolation='bilinear',  # resize를 할 때, interpolatrion 기법을 결정합니다.\n",
    "        color_mode ='rgb',\n",
    "        shuffle='True', # 이미지를 셔플링할 지 하지 않을 지를 결정.\n",
    "        class_mode='categorical') # multiclass의 경우이므로 class mode는 categorical\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train data의 파이프 라인이 batch size만큼의 데이터를 잘 불러오는 지 확인해 봅시다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('배치 데이터 크기:', data_batch.shape)\n",
    "    print('배치 레이블 크기:', labels_batch.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "바탕이 되는 Pretrained Model(ResNet50)을 불러오고 모델의 구조를 살펴봅시다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## back bone\n",
    "conv_base=tf.keras.applications.ResNet50(weights='imagenet',include_top=False)\n",
    "conv_base.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "최종 모델을 구성합니다.\n",
    "\n",
    "input layer와 ResNet50 backbone, fully-connected layer를 연결하여 transfer learning 모델을 만듭니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 최종 모델 구성하기\n",
    "input_layer = tf.keras.layers.Input(shape=(150,150,3))\n",
    "x = conv_base(input_layer) # 위에서 불러온 pretrained model을 활용하기\n",
    "# 불러온 conv_base 모델의 최종 결과물은 Conv2D 연산의 feature map과 동일\n",
    "# 따라서 최종적인 Multiclass classfication을 하기 위해서는 Flatten을 해야 한다.\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "out_layer = tf.keras.layers.Dense(4, activation='softmax')(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "conv_base는 freeze 시킴으로써 이미 학습된 파라미터 값을 그대로 사용합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "만들어진 모델의 구조를 살펴봅시다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input_layer], outputs=[out_layer])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# conv_base.trainable의 값을 True와 False로 바꿔가면서\n",
    "# Trainable params의 값이 어떻게 바뀌나 확인하고 분석해 보세요.\n",
    "# conv_base.trainable = True\n",
    "# model = tf.keras.Model(inputs=[input_layer], outputs=[out_layer])\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "loss function과 optimizer, metric을 설정하고 모델을 컴파일합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_function=tf.keras.losses.categorical_crossentropy\n",
    "optimize=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "metric=tf.keras.metrics.categorical_accuracy\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimize,\n",
    "              metrics=[metric])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "data generator는 입력 데이터와 타겟(라벨)의 batch를 끝없이 반환합니다.\n",
    "\n",
    "batch가 끝없이 생성되기 때문에, 한 번의 epoch에 generator로부터 얼마나 많은 샘플을 뽑을지 모델에 전달해야 합니다.\n",
    "\n",
    "만약 batch_size=20이고 steps_per_epoch=100일 경우 (데이터, 라벨)의 쌍 20개가 생성되고, 크기가 20인 batch 데이터를 100번 학습하면 1 epoch이 완료됩니다. 단, 크기 20의 batch 데이터는 매번 랜덤으로 생성됩니다.\n",
    "\n",
    "일반적으로 (전체 데이터 길이/batch_size)를 steps_per_epoch으로 설정합니다.\n",
    "\n",
    "참고: 모델 학습 시간이 30분 정도 걸립니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "     steps_per_epoch=(len(os.listdir(train_aeroplane_dir)) + len(os.listdir(train_bird_dir)) + len(\n",
    "       os.listdir(train_car_dir)) + len(os.listdir(train_cat_dir))) // batch_size,\n",
    "      epochs=20,\n",
    "      validation_data=test_generator,\n",
    "      validation_freq=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델에서 학습한 결과를 hdf5 파일 형식으로 저장하고, 평가 metric들도 따로 저장합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('/aiffel/aiffel/cifar_10_small/multi_classification_augumentation_model.hdf5')\n",
    "\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "training set과 validation set의 accuracy, loss를 그래프로 확인해봅시다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 학습한 결과 시각화\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img07/img14.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. Transfer learning의 목적에 대해서 설명해 보세요.\n",
    "\n",
    "Transfer learning은 데이터의 양과 연산 비용의 한계를 극복하면서 딥러닝 구조의 장점을 이용하기 위해서 고안되었습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. Transfer learning을 적용함에 있어서 어떤 기준으로 적용을 해야 하는지 설명해 보세요.\n",
    "\n",
    "Domain의 유사성과 가지고 있는 Dataset의 size입니다. Transfer learning을 적절하게 사용하기 위해서는 Pretrained model에서 어느 정도를 더 학습할지를 판단할 수 있어야 합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
