{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 딥러닝으로 시작하는 컴퓨터 비전\n",
    "### 9. Segmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9-1. 들어가며"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/01.png)\n",
    "![](./img09/02.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 학습 내용\n",
    "\n",
    "Semantic Segmentation vs. Instance Segmentation\n",
    "\n",
    " - Segmentation이 어떤 task 인지 이해하고 Segmentation의 종류를 나눠서 각 개념에 대해 알아봅시다.\n",
    "\n",
    "U-Net 모델을 통해 Semantic Segmentation 이해하기\n",
    "\n",
    " - Segmentation 중에서도 Semantic segmentation의 목표를 알아봅시다. 또한, U-Net 모델의 구성요소를 살펴보며 Semantic Segmentation을 이해해 봅시다.\n",
    "\n",
    "U-Net 코드를 통해서 이해 다지기\n",
    "\n",
    " - U-Net 모델을 직접 코드로 구현해 보고 다시 한번 Semantic Segmentation 이해하고 Encoder-Decoder 구조를 알아봅시다.\n",
    "\n",
    "학습 목표\n",
    "\n",
    "Segmentation의 목표를 이해하고, semantic segmentation과 instance segmentation의 차이점을 설명할 수 있습니다.\n",
    "\n",
    "Semantic segmentation의 대표적인 모델인 U-Net 모델의 구성 요소를 설명할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9-2. Semantic Segmentation vs Instance Segmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/03.png)\n",
    "\n",
    "Semantic Segmentation\n",
    "\n",
    " - Semantic Segmentation은 이미지에서 픽셀 단위로 분류하는 방법입니다. 이때, 같은 라벨은 같은 색으로 표시됩니다. Semantic Segmentation은 이미지에서 픽셀 단위로 분류하는 방법입니다. 이때, 같은 라벨은 같은 색으로 표시됩니다.\n",
    " - 하나의 이미지 안에 들어있는 객체의 종류를 (object category)를 픽셀 단위로 찾자\n",
    "\n",
    "Instance Segmentation\n",
    " - Instance Segmentation은 Semantic Segmentation과 비슷하지만 같은 라벨이라도 각각 다른 색으로 표시됩니다. Semantic Segmentation은 이미지에서 픽셀 단위로 분류하는 방법입니다. 이때, 같은 라벨은 같은 색으로 표시됩니다.\n",
    " - 하나의 이미지 안에 들어있는 객체의 객체를(object instance) 픽셀 단위로 찾자\n",
    "\n",
    "Instance Segmentation = Semantic Segmentation + distinguishing instance\n",
    "\n",
    "두 방법 다 데이터의 크기가 매우 크기 때문에 data augmentation이 필수적입니다.(소량의 데이터로 효과적인 방법을 찾는것)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. Segmentation는 어떤 task 인지 설명해 보세요.\n",
    "\n",
    "segmentation은 이미지를 픽셀 단위로 나누어서 특정 픽셀이 무엇을 지칭하는지를 파악하는 task 입니다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. Segmentation의 종류는 Semantic Segmentation과 Instance Segmentation으로 나눌 수 있습니다. 어떤 차이가 있는지 각 개념을 설명해 보세요.\n",
    "\n",
    "semantic segmentation은 하나의 이미지 안에 들어있는 객체의 종류(object category)를 픽셀 단위로 찾습니다. 반면 instance segmentation은 하나의 이미지 안에 들어있는 객체의 개체(object instance)를 픽셀 단위로 찾습니다.\n",
    "\n",
    "어떤 Segmentation 종류이든 이미지 데이터를 segmentation 모델 학습에 사용하기 위해서는 픽셀 하나하나 labeling을 해줘야 하기 때문에 데이터셋 구축이 어렵고, 따라서 data augmentation이 매우 중요합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9-3. U-Net 구조를 통해서 Segmentation 이해하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/04.png)\n",
    "\n",
    "Semantic Segmentation의 목표는 정답값(label)을 맞추는 것\n",
    "\n",
    "의료 데이터의 경우 정상과 비정상 2개로 나뉠수 있다. 하지만 예시의 경우 라벨값이 2개 이상일 경우 1부터 5까지 표시됨\n",
    "\n",
    "라벨을 픽셀단위로 Class를 픽셀단위로 분류하는 것이 Semantic Segmentation의 목표"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/05.png)\n",
    "\n",
    "정답은 위와 같이 구성되어 있다. (0과 1의 Binary Encoding으로 구성되어 있다)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/06.png)\n",
    "\n",
    "여러개의 인풋을 가진 채널 존재. 아웃풋은 강우량(양의 실수)\n",
    "\n",
    "이것은 regression task로 U-Net은 regression task도 수행 가!\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/07.png)\n",
    "\n",
    "Transpoesd Convolution(Encorder, Decorder)에 skip Connection을 추가한 구조가 U-Net\n",
    "\n",
    "Skip Connection은 ResNet에서 처음 등장했고, U-Net에서도 사용되었다 (ResNet은 Classification을 위한 모델, Gradient HighWay)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/08.png)\n",
    "\n",
    "Up conv = Transposed Convolution\n",
    "\n",
    "3*3+Relu conv을 연달아 2번 진행\n",
    "\n",
    "회색 화살표의 Crop and Copy 의미 : Down sampling (U를 반으로 잘랐을때 왼쪽편)에서 그림 상 점선의 값이 오른편으로 이동한다.\n",
    "\n",
    "ex) 세번째 층 외편의 점선을 보면 점선의 값이 그대로 오른편으로 이동하게 된다.\n",
    "\n",
    "피쳐맵의 전체를 사용하는게 아니라 136^2에서 104^2만 잘라내서(crop) 오른편의 동일층에 붙인다(concat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/09.png)\n",
    "\n",
    "U-Net의 왼편은 Encoder(압축)만 가지고 Semantic Segmentation을 수행할 수 없다.\n",
    "\n",
    "이미지 위치에 대한 정보가 점점 소실되기 때문\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/10.png)\n",
    "\n",
    "압축된 이미지를 High Resolution으로 복원\n",
    "\n",
    "대부분의 semantic segmentation 인풋사이즈와 아웃풋 사이즈가 동일하지만 U-Net은 아웃풋이 더 작다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./img09/11.png)\n",
    "\n",
    "Concat을 통해 인코딩 과정에서의 정보 손실을 보충한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. 앞에서 Segmentation의 종류 2가지를 배웠습니다. 그중에서 Semantic segmentation의 목표는 무엇인지 설명해 보세요.\n",
    "\n",
    "Semantic segmentation의 목표는 이미지가 주어졌을 때, 픽셀 단위로 Classification을 수행하여 이미지와 동일한 높이와 너비를 가진 Segmentation map을 생성하는 것입니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. U-Net 모델은 Encoder-Decoder 모델에 skip connection을 추가한 모델이라고 할 수 있습니다. U-Net 모델 구조 중 Encoder에 해당하는 Contracting path 부분의 특징을 설명해 보세요.\n",
    "\n",
    "convolution 연산으로 이루어진 부분이며 CNN 구조와 유사하다는 특징이 있습니다. 또한 3x3 kernel을 사용하는 VGG 모델과 매우 유사하며 입력 이미지가 가지고 있는 context 정보를 추출합니다. U-Net 모델 구조 중 Encoder에 해당하며 convolution 연산을 하기 때문에 이미지의 위치에 대한 정보가 차츰 사라집니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. U-Net 모델 구조 중 Decoder에 해당하는 Expanding (Expansive) path 부분의 특징을 설명해 보세요.\n",
    "\n",
    "up-convolution 연산으로 이루어진 부분이며 low resolution의 latent representation을 high resolution으로 변형합니다. 또한 Encoder 부분인 contracting path에서 만들어진 feature map을 cropping 한 결과물이 concatenation 됩니다. U-Net 모델 구조 중 Decoder에 해당하며 up-convolution 연산을 하기 때문에 원본 이미지가 가지고 있었던 위치 정보가 복원됩니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q. U-Net 모델은 Encoding 부분에서 이미지를 압축하며 정보의 손실이 발생합니다. 이를 보완하기 위해 어떤 방법을 사용하는지, 그 방법의 특징은 무엇인 설명해 보세요.\n",
    "\n",
    "Encoding 과정에서의 정보 손실을 보충하기 위해 skip connection을 사용합니다. 이것은 Decoding 단계에서, 저차원의 정보와 고차원의 정보도 함께 이용하기 위한 방법입니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9-4. U-Net 코드를 통해서 이해 다지기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "U-Net 모델의 코드를 보면서 구조를 이해해봅시다.\n",
    "먼저 시각화에 필요한 라이브러리들을 설치하고 필요한 라이브러리를 불러옵니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /Users/ralphpark/anaconda3/lib/python3.10/site-packages (0.20.1)\r\n",
      "Requirement already satisfied: pydot in /Users/ralphpark/anaconda3/lib/python3.10/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /Users/ralphpark/anaconda3/lib/python3.10/site-packages (from pydot) (3.0.9)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "!pip install pydot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T09:39:07.181276Z",
     "start_time": "2023-08-16T09:38:59.165520Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 18:39:09.242608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T09:39:14.145309Z",
     "start_time": "2023-08-16T09:39:07.182145Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "U-Net 모델을 구현한 코드입니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/cv-b-9-4-1.max-800x600.png)\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/cv-b-9-4-2.max-800x600.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(572, 572, 1))\n",
    "\n",
    "# Contracting path 시작\n",
    "# [1]\n",
    "conv0 = layers.Conv2D(64, activation='relu', kernel_size = 3)(inputs)\n",
    "conv1 = layers.Conv2D(64, activation='relu', kernel_size=3)(conv0)  # Skip connection으로 Expanding path로 이어질 예정\n",
    "conv2 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n",
    "\n",
    "\n",
    "# Q.위 이미지를 보고 [2]번 블럭을 구현해 봅시다. (filter 수를 주의하세요!)\n",
    "conv3 = layers.Conv2D(128, activation='relu', kernel_size=3)(conv2)\n",
    "conv4 = layers.Conv2D(128, activation='relu', kernel_size=3)(conv3)\n",
    "conv5 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv4)\n",
    "\n",
    "\n",
    "# Q.위 이미지를 보고 [3]번 블럭을 구현해 봅시다. (filter 수를 주의하세요!)\n",
    "conv6 = layers.Conv2D(256, activation='relu', kernel_size=3)(conv5)\n",
    "conv7 = layers.Conv2D(256, activation='relu', kernel_size=3)(conv6)\n",
    "conv8 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv7)\n",
    "\n",
    "\n",
    "# Q.위 이미지를 보고 [4]번 블럭을 구현해 봅시다. (filter 수를 주의하세요!)\n",
    "conv9 = layers.Conv2D(512, activation='relu', kernel_size=3)(conv8)\n",
    "conv10 = layers.Conv2D(512, activation='relu', kernel_size=3)(conv9)\n",
    "conv11 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv10)\n",
    "\n",
    "\n",
    "# [5]\n",
    "conv12 = layers.Conv2D(1024, activation='relu', kernel_size=3)(conv11)\n",
    "conv13 = layers.Conv2D(1024, activation='relu', kernel_size=3)(conv12)\n",
    "# Contracting path 끝\n",
    "\n",
    "# Expanding path 시작\n",
    "# [6]\n",
    "trans01 = layers.Conv2DTranspose(512, kernel_size=2, strides=(2, 2), activation='relu')(conv13)\n",
    "crop01 = layers.Cropping2D(cropping=(4, 4))(conv10)\n",
    "concat01 = layers.concatenate([trans01, crop01], axis=-1)\n",
    "\n",
    "# [7]\n",
    "conv14 = layers.Conv2D(512, activation='relu', kernel_size=3)(concat01)\n",
    "conv15 = layers.Conv2D(512, activation='relu', kernel_size=3)(conv14)\n",
    "trans02 = layers.Conv2DTranspose(256, kernel_size=2, strides=(2, 2), activation='relu')(conv15)\n",
    "\n",
    "# [8]\n",
    "crop02 = layers.Cropping2D(cropping=(16, 16))(conv7)\n",
    "concat02 = layers.concatenate([trans02, crop02], axis=-1)\n",
    "\n",
    "\n",
    "# Q.위 이미지를 보고 [9]번 블럭을 구현해 봅시다. (filter 수를 주의하세요!)\n",
    "conv16 = layers.Conv2D(256, activation='relu', kernel_size=3)(concat02)\n",
    "conv17 = layers.Conv2D(256, activation='relu', kernel_size=3)(conv16)\n",
    "trans03 = layers.Conv2DTranspose(128, kernel_size=2, strides=(2, 2), activation='relu')(conv17)\n",
    "\n",
    "\n",
    "# Q.위 이미지를 보고 [10]번 블럭을 구현해 봅시다. (cropping=(40, 40))\n",
    "crop03 = layers.Cropping2D(cropping=(40, 40))(conv4)\n",
    "concat03 = layers.concatenate([trans03, crop03], axis=-1)\n",
    "\n",
    "\n",
    "# Q.위 이미지를 보고 [11]번 블럭을 구현해 봅시다. (filter 수를 주의하세요!)\n",
    "conv18 = layers.Conv2D(128, activation='relu', kernel_size=3)(concat03)\n",
    "conv19 = layers.Conv2D(128, activation='relu', kernel_size=3)(conv18)\n",
    "trans04 = layers.Conv2DTranspose(64, kernel_size=2, strides=(2, 2), activation='relu')(conv19)\n",
    "\n",
    "\n",
    "# Q.위 이미지를 보고 [12]번 블럭을 구현해 봅시다. (cropping=(88, 88))\n",
    "crop04 = layers.Cropping2D(cropping=(88, 88))(conv1)\n",
    "concat04 = layers.concatenate([trans04, crop04], axis=-1)\n",
    "\n",
    "\n",
    "# [13]\n",
    "conv20 = layers.Conv2D(64, activation='relu', kernel_size=3)(concat04)\n",
    "conv21 = layers.Conv2D(64, activation='relu', kernel_size=3)(conv20)\n",
    "# Expanding path 끝\n",
    "\n",
    "outputs = layers.Conv2D(2, kernel_size=1)(conv21)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"u-netmodel\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T09:39:14.506957Z",
     "start_time": "2023-08-16T09:39:14.153761Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"u-netmodel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 572, 572, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 570, 570, 64)         640       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 568, 568, 64)         36928     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 284, 284, 64)         0         ['conv2d_1[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 282, 282, 128)        73856     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 280, 280, 128)        147584    ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 140, 140, 128)        0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 138, 138, 256)        295168    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 136, 136, 256)        590080    ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 68, 68, 256)          0         ['conv2d_5[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 66, 66, 512)          1180160   ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 64, 64, 512)          2359808   ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 32, 32, 512)          0         ['conv2d_7[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 30, 30, 1024)         4719616   ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 1024)         9438208   ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 56, 56, 512)          2097664   ['conv2d_9[0][0]']            \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " cropping2d (Cropping2D)     (None, 56, 56, 512)          0         ['conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 56, 56, 1024)         0         ['conv2d_transpose[0][0]',    \n",
      "                                                                     'cropping2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 54, 54, 512)          4719104   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 52, 52, 512)          2359808   ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 104, 104, 256)        524544    ['conv2d_11[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " cropping2d_1 (Cropping2D)   (None, 104, 104, 256)        0         ['conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 104, 104, 512)        0         ['conv2d_transpose_1[0][0]',  \n",
      " )                                                                   'cropping2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 102, 102, 256)        1179904   ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 100, 100, 256)        590080    ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 200, 200, 128)        131200    ['conv2d_13[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " cropping2d_2 (Cropping2D)   (None, 200, 200, 128)        0         ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 200, 200, 256)        0         ['conv2d_transpose_2[0][0]',  \n",
      " )                                                                   'cropping2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 198, 198, 128)        295040    ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 196, 196, 128)        147584    ['conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 392, 392, 64)         32832     ['conv2d_15[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " cropping2d_3 (Cropping2D)   (None, 392, 392, 64)         0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 392, 392, 128)        0         ['conv2d_transpose_3[0][0]',  \n",
      " )                                                                   'cropping2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 390, 390, 64)         73792     ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 388, 388, 64)         36928     ['conv2d_16[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 388, 388, 2)          130       ['conv2d_17[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31030658 (118.37 MB)\n",
      "Trainable params: 31030658 (118.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T09:39:14.589097Z",
     "start_time": "2023-08-16T09:39:14.508811Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "위에서 구현한 U-Net 모델의 구조를 그림으로 나타낼 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.utils.vis_utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SVG\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvis_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m model_to_dot\n\u001B[1;32m      4\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatplotlib\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minline\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m SVG(model_to_dot(model, show_shapes\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, show_layer_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, dpi\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m80\u001B[39m)\u001B[38;5;241m.\u001B[39mcreate(prog\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdot\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msvg\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'keras.utils.vis_utils'"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=80).create(prog='dot', format='svg'))  #dpi를 작게 하면 그래프가 커집니다."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T09:39:14.979886Z",
     "start_time": "2023-08-16T09:39:14.584451Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
